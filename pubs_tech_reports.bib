@article{Banerjee2013,
 abstract = {An extremely common bottleneck encountered in statistical learning algorithms is inversion of huge covariance matrices, examples being in evaluating Gaussian likelihoods for a large number of data points. We propose general parallel algorithms for inverting positive definite matrices, which are nearly rank deficient. Such matrix inversions are needed in Gaussian process computations, among other settings, and remain a bottleneck even with the increasing literature on low rank approximations. We propose a general class of algorithms for parallelizing computations to dramatically speed up computation time by orders of magnitude exploiting multicore architectures. We implement our algorithm on a cloud computing platform, providing pseudo and actual code. The algorithm can be easily implemented on any multicore parallel computing resource. Some illustrations are provided to give a flavor for the gains and what becomes possible in freeing up this bottleneck.},
 author = {Banerjee, Anjishnu and Vogelstein, Joshua and Dunson, David},
 eprint = {1312.1869},
 journal = {arXiv},
 keywords = {big data,gaussian process,mapreduce,matrix inversion,parallel computing,qr decom-},
 title = {{Parallel inversion of huge covariance matrices}},
 url = {http://arxiv.org/abs/1312.1869},
 volume = {1312.1869},
 year = {2013}
}

@article{Kazhdan2013,
 abstract = {We propose a new gradient-domain technique for processing registered EM image stacks to remove the inter-image discontinuities while preserving intra-image detail. To this end, we process the image stack by first performing anisotropic diffusion to smooth the data along the slice axis and then solving a screened-Poisson equation within each slice to re-introduce the detail. The final image stack is both continuous across the slice axis (facilitating the tracking of information between slices) and maintains sharp details within each slice (supporting automatic feature detection). To support this editing, we describe the implementation of the first multigrid solver designed for efficient gradient domain processing of large, out-of-core, voxel grids.},
 author = {Kazhdan, Michael and Burns, Randal and Kasthuri, Bobby and Lichtman, Jeff and Vogelstein, Jacob and Vogelstein, Joshua},
 eprint = {1310.0041},
 journal = {arXiv},
 month = {sep},
 title = {{Gradient-Domain Processing for Large EM Image Stacks}},
 url = {http://arxiv.org/abs/1310.0041},
 year = {2013}
}

@article{kiar2018neurostorm,
 abstract = {Neuroscientists are now able to acquire data at staggering rates across spatiotemporal scales. However, our ability to capitalize on existing datasets, tools, and intellectual capacities is hampered by technical challenges. The key barriers to accelerating scientific discovery correspond to the FAIR data principles: findability, global access to data, software interoperability, and reproducibility/re-usability. We conducted a hackathon dedicated to making strides in those steps. This manuscript is a technical report summarizing these achievements, and we hope serves as an example of the effectiveness of focused, deliberate hackathons towards the advancement of our quickly-evolving field.},
 author = {Kiar, Gregory and Anderson, Robert J. and Baden, Alex and Badea, Alexandra and Bridgeford, Eric W. and Champion, Andrew and Chandrashekhar, Vikram and Collman, Forrest and Duderstadt, Brandon and Evans, Alan C. and Engert, Florian and Falk, Benjamin and Glatard, Tristan and Roncal, William R. Gray and Kennedy, David N. and Maitin-Shepard, Jeremy and Marren, Ryan A. and Nnaemeka, Onyeka and Perlman, Eric and Seshamani, Sharmishtaas and Trautman, Eric T. and Tward, Daniel J. and Valdés-Sosa, Pedro Antonio and Wang, Qing and Miller, Michael I. and Burns, Randal and Vogelstein, Joshua T.},
 eprint = {1803.03367},
 journal = {arXiv},
 month = {mar},
 title = {{NeuroStorm: Accelerating Brain Science Discovery in the Cloud}},
 url = {http://arxiv.org/abs/1803.03367},
 year = {2018}
}

@article{Priebe2017,
 abstract = {We present semiparametric spectral modeling of the complete larval Drosophila mushroom body connectome. Motivated by a thorough exploratory data analysis of the network via Gaussian mixture modeling (GMM) in the adjacency spectral embedding (ASE) representation space, we introduce the latent structure model (LSM) for network modeling and inference. LSM is a generalization of the stochastic block model (SBM) and a special case of the random dot product graph (RDPG) latent position model, and is amenable to semiparametric GMM in the ASE representation space. The resulting connectome code derived via semiparametric GMM composed with ASE captures latent connectome structure and elucidates biologically relevant neuronal properties.},
 author = {Priebe, Carey E. and Park, Youngser and Tang, Minh and Athreya, Avanti and Lyzinski, Vince and Vogelstein, Joshua T. and Qin, Yichen and Cocanougher, Ben and Eichler, Katharina and Zlatic, Marta and Cardona, Albert},
 eprint = {1705.03297},
 journal = {arXiv},
 title = {{Semiparametric spectral modeling of the Drosophila connectome}},
 url = {http://arxiv.org/abs/1705.03297},
 year = {2017}
}

@article{sinha2014automatic,
 author = {Sinha, Ayushi and Roncal, William Gray and Kasthuri, Narayanan and Chuang, Ming and Manavalan, Priya and Kleissas, Dean M and Vogelstein, Joshua T. and Vogelstein, R Jacob and Burns, Randal and Lichtman, Jeff W and others},
 journal = {arXiv},
 title = {Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes},
 url = {https://arxiv.org/abs/1404.4800},
 year = {2014}
}

@article{Zheng2016,
 abstract = {—FlashMatrix is a matrix-oriented programming framework for general data analysis with high-level functional programming interface. It scales matrix operations beyond mem-ory capacity by utilizing solid-state drives (SSDs) in non-uniform memory architecture (NUMA). It provides a small number of generalized matrix operations (GenOps) and reimplements a large number of matrix operations in the R framework with GenOps. As such, it executes R code in parallel and out of core automatically. FlashMatrix uses vectorized user-defined functions (VUDF) to reduce the overhead of function calls and fuses matrix operations to reduce data movement between CPU and SSDs. We implement multiple machine learning algorithms in R to benchmark the performance of FlashMatrix. The execution of the R implementations in FlashMatrix has performance comparable to optimized C implementations. When scaling beyond memory capacity on a large parallel machine, the out-of-core execution of these R implementations in FlashMatrix has performance com-parable to in-memory execution on a billion-scale dataset. Both in-memory and out-of-core execution significantly outperform the in-memory execution of Spark MLlib.},
 author = {Zheng, Da and Mhembere, Disa and Vogelstein, Joshua and Priebe, Carey E and Burns, Randal},
 eprint = {arXiv},
 journal = {arXiv},
 title = {{FlashMatrix: Parallel, Scalable Data Analysis with Generalized Matrix Operations using Commodity SSDs}},
 url = {http://arxiv.org/abs/1604.06414v1},
 year = {2016}
}

@article{Zheng2016c,
 abstract = {Many eigensolvers such as ARPACK and Anasazi have been developed to compute eigenvalues of a large sparse matrix. These eigensolvers are limited by the capacity of RAM. They run in memory of a single machine for smaller eigenvalue problems and require the distributed memory for larger problems. In contrast, we develop an SSD-based eigensolver framework called FlashEigen, which extends Anasazi eigensolvers to SSDs, to compute eigenvalues of a graph with hundreds of millions or even billions of vertices in a single machine. FlashEigen performs sparse matrix multiplication in a semi-external memory fashion, i.e., we keep the sparse matrix on SSDs and the dense matrix in memory. We store the entire vector subspace on SSDs and reduce I/O to improve performance through caching the most recent dense matrix. Our result shows that FlashEigen is able to achieve 40%-60% performance of its in-memory implementation and has performance comparable to the Anasazi eigensolvers on a machine with 48 CPU cores. Furthermore, it is capable of scaling to a graph with 3.4 billion vertices and 129 billion edges. It takes about four hours to compute eight eigenvalues of the billion-node graph using 120 GB memory.},
 author = {Zheng, Da and Burns, Randal and Vogelstein, Joshua and Priebe, Carey E. and Szalay, Alexander S.},
 eprint = {1602.01421},
 journal = {arXiv},
 title = {{An SSD-based eigensolver for spectral analysis on billion-node graphs}},
 url = {http://arxiv.org/abs/1602.01421},
 year = {2016}
}

