@article{Browne2018,
 abstract = {Decision Forests are popular machine learning techniques that assist scientists to extract knowledge from massive data sets. This class of tool remains popular because of their interpretability and ease of use, unlike other modern machine learning methods, such as kernel machines and deep learning. Decision forests also scale well for use with large data because training and run time operations are trivially parallelizable allowing for high inference throughputs. A negative aspect of these forests, and an untenable property for many real time applications, is their high inference latency caused by the combination of large model sizes with random memory access patterns. We present memory packing techniques and a novel tree traversal method to overcome this deficiency. The result of our system is a grouping of trees into a hierarchical structure. At low levels, we pack the nodes of multiple trees into contiguous memory blocks so that each memory access fetches data for multiple trees. At higher levels, we use leaf cardinality to identify the most popular paths through a tree and collocate those paths in contiguous cache lines. We extend this layout with a re-ordering of the tree traversal algorithm to take advantage of the increased memory throughput provided by out-of-order execution and cache-line prefetching. Together, these optimizations increase the performance and parallel scalability of classification in ensembles by a factor of ten over an optimized C++ implementation and a popular R-language implementation.},
 archiveprefix = {arXiv},
 arxivid = {1806.07300},
 author = {Browne, James and Mhembere, Disa and Tomita, Tyler M. and Vogelstein, Joshua T. and Burns, Randal},
 author+an = {1=trainee;2=trainee;3=trainee;4=highlinght},
 doi = {10.1137/1.9781611975673.6},
 eprint = {1806.07300},
 isbn = {9781611975673},
 journal = {SIAM International Conference on Data Mining, SDM},
 month = {jun},
 pages = {46--54},
 title = {{Forest packing: Fast Parallel, Decision Forests}},
 url = {https://arxiv.org/abs/1806.07300},
 year = {2019}
}

@inproceedings{burns2013open,
 abstract = {We describe a scalable database cluster for the spatial analysis and annotation of high-throughput brain imaging data, initially for 3-d electron microscopy image stacks, but for time-series and multi-channel data as well. The system was designed primarily for workloads that build connectomes---neural connectivity maps of the brain---using the parallel execution of computer vision algorithms on high-performance compute clusters. These services and open-science data sets are publicly available at http://openconnecto.me. The system design inherits much from NoSQL scale-out and data-intensive computing architectures. We distribute data to cluster nodes by partitioning a spatial index. We direct I/O to different systems---reads to parallel disk arrays and writes to solid-state storage---to avoid I/O interference and maximize throughput. All programming interfaces are RESTful Web services, which are simple and stateless, improving scalability and usability. We include a performance evaluation of the production system, highlighting the effectiveness of spatial data organization.},
 archiveprefix = {arXiv},
 arxivid = {1306.3543},
 author = {Burns, Randal and Roncal, William Gray and Kleissas, Dean and Lillaney, Kunal and Manavalan, Priya and Perlman, Eric and Berger, Daniel R. and Bock, Davi D. and Chung, Kwanghun and Grosenick, Logan and Kasthuri, Narayanan and Weiler, Nicholas C. and Deisseroth, Karl and Kazhdan, Michael and Lichtman, Jeff and Reid, R. Clay and Smith, Stephen J. and Szalay, Alexander S. and Vogelstein, Joshua T. and Vogelstein, R. Jacob},
 author+an = {2=trainee;4=trainee;19=highlight},
 booktitle = {ACM International Conference Proceeding Series},
 doi = {10.1145/2484838.2484870},
 eprint = {1306.3543},
 isbn = {978-1-4503-1921-8},
 issn = {15378276},
 keywords = {Connectomics,Data-intensive computing},
 organization = {ACM},
 pmid = {24401992},
 title = {{The Open Connectome Project Data Cluster: Scalable Analysis and Vision for High-Throughput Neuroscience}},
 url = {http://arxiv.org/abs/1306.3543},
 year = {2013}
}

@article{Carlson2013a,
 abstract = {With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-the-art. Via exploratory data analysis-using data with partial ground truth as well as two novel data sets-we find several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain.},
 author = {Carlson, David E and Rao, Vinayak and Vogelstein, Joshua T and Carin, Lawrence},
 author+an = {3=highlight},
 issn = {10495258},
 journal = {Advances in Neural Information Processing Systems 26},
 title = {{Real-Time Inference for a Gamma Process Model of Neural Spiking}},
 url = {http://papers.nips.cc/paper/5061-real-time-inference-for-a-gamma-process-model-of-neural-spiking.pdf},
 year = {2013}
}

@article{Cornelis2013,
 abstract = {The preservation of our cultural heritage is of paramount importance. Thanks to recent developments in digital acquisition techniques, powerful image analysis algorithms are developed which can be useful non-invasive tools to assist in the restoration and preservation of art. In this paper we propose a semi-supervised crack detection method that can be used for high-dimensional acquisitions of paintings coming from different modalities. Our dataset consists of a recently acquired collection of images of the Ghent Altarpiece (1432), one of Northern Europe's most important art masterpieces. Our goal is to build a classifier that is able to discern crack pixels from the background consisting of non-crack pixels, making optimal use of the information that is provided by each modality. To accomplish this we employ a recently developed non-parametric Bayesian classifier, that uses tensor factorizations to characterize any conditional probability. A prior is placed on the parameters of the factorization such that every possible interaction between predictors is allowed while still identifying a sparse subset among these predictors. The proposed Bayesian classifier, which we will refer to as conditional Bayesian tensor factorization or CBTF, is assessed by visually comparing classification results with the Random Forest (RF) algorithm. {\textcopyright} 2013 IEEE.},
 author = {Cornelis, Bruno and Yang, Yun and Vogelstein, Joshua T. and Dooms, Ann and Daubechies, Ingrid and Dunson, David},
 author+an = {3=highlight},
 doi = {10.1109/ICDSP.2013.6622710},
 eprint = {1304.5894},
 isbn = {9781467358057},
 journal = {18th International Conference on Digital Signal Processing},
 keywords = {Classification,Crack detection,Ghent altarpiece,Nonparametric Bayes,Tensor factorization,Variable selection},
 title = {{Bayesian crack detection in ultra high resolution multimodal images of paintings}},
 url = {http://arxiv.org/abs/1304.5894},
 year = {2013}
}

@article{Fiori2013,
 abstract = {Graph matching is a challenging problem with very important applications in a wide range of fields, from image and video analysis to biological and biomedical problems. We propose a robust graph matching algorithm inspired in sparsity-related techniques. We cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efficiently solved using augmented Lagrangian techniques. The method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data. The proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence. The algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs. We also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fMRI) data. The code is publicly available.},
 annote = {(spotlight)},
 author = {Fiori, Marcelo and Sprechmann, Pablo and Vogelstein, Joshua and Muse, Pablo and Sapiro, Guillermo},
 author+an = {3=highlight},
 eprint = {arXiv},
 isbn = {10.1186/2042-1001-1-16},
 issn = {10495258},
 journal = {Advances in Neural Information Processing Systems},
 pmid = {25233306},
 title = {{Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching}},
 url = {http://papers.nips.cc/paper/4925-robust-multimodal-graph-matching-sparse-coding-meets-graph-matching},
 year = {2013}
}

@inproceedings{GrayRoncal2013,
 abstract = {Currently, connectomes (e.g., functional or structural brain graphs) can be estimated in humans at â‰ˆ 1 mm3 scale using a combination of diffusion weighted magnetic resonance imaging, functional magnetic resonance imaging and structural magnetic resonance imaging scans. This manuscript summarizes a novel, scalable implementation of open-source algorithms to rapidly estimate magnetic resonance connectomes, using both anatomical regions of interest (ROIs) and voxel-size vertices. To assess the reliability of our pipeline, we develop a novel non-parametric non-Euclidean reliability metric. Here we provide an overview of the methods used, demonstrate our implementation, and discuss available user extensions. We conclude with results showing the efficacy and reliability of the pipeline over previous state-of-the-art.},
 author = {Roncal, William Gray and Koterba, Zachary H. and Mhembere, Disa and Kleissas, Dean M. and Vogelstein, Joshua T. and Burns, Randal and Bowles, Anita R. and Donavos, Dimitrios K. and Ryman, Sephira and Jung, Rex E. and Wu, Lei and Calhoun, Vince and Vogelstein, R. Jacob},
 author+an = {1=trainee;3=trainee;5=highlight},
 booktitle = {2013 IEEE Global Conference on Signal and Information Processing},
 doi = {10.1109/GlobalSIP.2013.6736878},
 eprint = {1312.4875},
 isbn = {9781479902484},
 keywords = {biodiffusion;biomedical MRI;graph theory;reliability;MIGRAINE;MRI graph reliability analysis and inference for connectomics;diffusion weighted magnetic resonance imaging;functional magnetic resonance imaging;structural magnetic resonance imaging scans;open-source algorithms;magnetic resonance connectomes estimation;anatomical regions of interest;ROIs;voxel-size vertices;reliability assessment;nonparametric nonEuclidean reliability metric;Pipelines;Magnetic resonance imaging;Educational institutions;Robustness;Scalability;Neuroscience;connectomics;magnetic resonance imaging;network theory;pipeline},
 month = {dec},
 pages = {313-316},
 publisher = {IEEE},
 title = {{MIGRAINE: MRI graph reliability analysis and inference for connectomics}},
 url = {http://ieeexplore.ieee.org/document/6736878/},
 year = {2013}
}

@inproceedings{GrayRoncal2015a,
 abstract = {An open challenge problem at the forefront of modern neuroscience is to obtain a comprehensive mapping of the neural pathways that underlie human brain function; an enhanced understanding of the wiring diagram of the brain promises to lead to new breakthroughs in diagnosing and treating neurological disorders. Inferring brain structure from image data, such as that obtained via electron microscopy (EM), entails solving the problem of identifying biological structures in large data volumes. Synapses, which are a key communication structure in the brain, are particularly difficult to detect due to their small size and limited contrast. Prior work in automated synapse detection has relied upon time-intensive biological preparations (post-staining, isotropic slice thicknesses) in order to simplify the problem. This paper presents VESICLE, the first known approach designed for mammalian synapse detection in anisotropic, non-post-stained data. Our methods explicitly leverage biological context, and the results exceed existing synapse detection methods in terms of accuracy and scalability. We provide two different approaches - one a deep learning classifier (VESICLE-CNN) and one a lightweight Random Forest approach (VESICLE-RF) to offer alternatives in the performance-scalability space. Addressing this synapse detection challenge enables the analysis of high-throughput imaging data soon expected to reach petabytes of data, and provide tools for more rapid estimation of brain-graphs. Finally, to facilitate community efforts, we developed tools for large-scale object detection, and demonstrated this framework to find {\$}\backslashapprox{\$} 50,000 synapses in 60,000 {\$}\backslashmu m {\^{}}3{\$} (220 GB on disk) of electron microscopy data.},
 archiveprefix = {arXiv},
 arxivid = {1403.3724},
 author = {Roncal, William Gray and Pekala, Michael and Kaynig-Fittkau, Verena and Kleissas, Dean M and Vogelstein, Joshua T and Pfister, Hanspeter and Burns, Randal and Vogelstein, R Jacob and Chevillet, Mark A and Hager, Gregory D},
 author+an = {1=trainee;5=highlight},
 booktitle = {British Machine Vision Conference},
 doi = {10.5244/c.29.81},
 eprint = {http://arxiv.org/abs/1403.3724},
 pages = {81.1--81.13},
 title = {{VESICLE: Volumetric Evaluation of Synaptic Inferfaces using Computer Vision at Large Scale}},
 year = {2015}
}

@article{Huys2009,
 author = {Huys, Quentin J and Vogelstein, Joshua and Dayan, Peter},
 author+an = {2=highlight},
 journal = {Advances in Neural Information Processing Systems},
 title = {Psychiatry: Insights into depression through normative decision-making models},
 url = {http://papers.nips.cc/paper/3563-psychiatry-insights-into-depression-through-normative-decision-making-models.pdf},
 year = {2008}
}

@article{Koutra2013,
 abstract = {How much did a network change since yesterday? How different is the wiring between Bob's brain (a left-handed male) and Alice's brain (a right-handed female)? Graph similarity with known node correspondence, i.e. the detection of changes in the connectivity of graphs, arises in numerous settings. In this work, we formally state the axioms and desired properties of the graph similarity functions, and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs. We propose DeltaCon, a principled, intuitive, and scalable algorithm that assesses the similarity between two graphs on the same nodes (e.g. employees of a company, customers of a mobile carrier). Experiments on various synthetic and real graphs showcase the advantages of our method over existing similarity measures. Finally, we employ DeltaCon to real applications: (a) we classify people to groups of high and low creativity based on their brain connectivity graphs, and (b) do temporal anomaly detection in the who-emails-whom Enron graph.},
 archiveprefix = {arXiv},
 arxivid = {1304.4657},
 author = {Koutra, Danai and Vogelstein, Joshua T. and Faloutsos, Christos},
 author+an = {2=highlight},
 chapter = {17},
 doi = {10.1137/1.9781611972832.18},
 eprint = {1304.4657},
 isbn = {9781611972627},
 issn = {1095-712X},
 journal = {Proceedings of the 2013 SIAM International Conference on Data Mining, SDM 2013},
 pages = {162--170},
 title = {{DELTACON: A principled massive-graph similarity function}},
 url = {http://arxiv.org/abs/1304.4657},
 year = {2013}
}

@inproceedings{Kulkarni2013,
 abstract = {The human brain and the neuronal networks comprising it are of immense interest to the scientific community. In this work, we focus on the structural connectivity of human brains, investigating sex differences across male and female connectomes (brain-graphs) for the knowledge discovery problem "Which brain regions exert differences in connectivity across the two sexes?". One of our main findings discloses the statistical difference at the pars orbitalis of the connectome between sexes, which has been shown to function in language production. Moreover, we use these discriminative regions for the related learning problem "Can we classify a given human connectome to belong to one of the sexes just by analyzing its connectivity structure?". We show that we can learn decision tree as well as support vector machine classification models for this task. We show that our models achieve up to 79{\%} prediction accuracy with only a handful of brain regions as discriminating factors. Importantly, our results are consistent across two data sets, collected at two different centers, with two different scanning sequences, and two different age groups (children and elderly). This is highly suggestive that we have discovered scientifically meaningful sex differences. {\textcopyright} Springer International Publishing 2013.},
 author = {Kulkarni, Vivek and Pudipeddi, Jagat Sastry and Akoglu, Leman and Vogelstein, Joshua T and Vogelstein, R Jacob and Ryman, Sephira and Jung, Rex E},
 author+an = {4=highlight},
 booktitle = {Brain and Health Informatics},
 doi = {10.1007/978-3-319-02753-1_9},
 isbn = {9783319027524},
 issn = {03029743},
 keywords = {Graph measures,Human connectome,Network connectivity,Network science,Pars orbitalis,Sex classification},
 organization = {Springer},
 pages = {82--91},
 title = {{Sex differences in the human connectome}},
 url = {https://pdfs.semanticscholar.org/98da/eeccc6d3cc80b789de30ecf8790c56950739.pdf},
 volume = {8211 LNAI},
 year = {2013}
}

@article{Kutten2016,
 abstract = {The CLARITY method renders brains optically transparent to enable high-resolution imaging in the structurally intact brain. Anatomically annotating CLARITY brains is necessary for discovering which regions contain signals of interest. Manually annotating whole-brain, terabyte CLARITY images is difficult, time-consuming, subjective, and error-prone. Automatically registering CLARITY images to a pre-annotated brain atlas offers a solution, but is difficult for several reasons. Removal of the brain from the skull and subsequent storage and processing cause variable non-rigid deformations, thus compounding inter-subject anatomical variability. Additionally, the signal in CLARITY images arises from various biochemical contrast agents which only sparsely label brain structures. This sparse labeling challenges the most commonly used registration algorithms that need to match image histogram statistics to the more densely labeled histological brain atlases. The standard method is a multiscale Mutual Information B-spline algorithm that dynamically generates an average template as an intermediate registration target. We determined that this method performs poorly when registering CLARITY brains to the Allen Institute's Mouse Reference Atlas (ARA), because the image histogram statistics are poorly matched. Therefore, we developed a method (Mask-LDDMM) for registering CLARITY images, that automatically find the brain boundary and learns the optimal deformation between the brain and atlas masks. Using Mask-LDDMM without an average template provided better results than the standard approach when registering CLARITY brains to the ARA. The LDDMM pipelines developed here provide a fast automated way to anatomically annotate CLARITY images. Our code is available as open source software at http://NeuroData.io.},
 archiveprefix = {arXiv},
 arxivid = {1605.02060},
 author = {Kutten, Kwame S. and Vogelstein, Joshua T. and Charon, Nicolas and Ye, Li and Deisseroth, Karl and Miller, Michael I.},
 author+an = {1=trainee;2=highlight},
 doi = {10.1117/12.2227444},
 eprint = {1605.02060},
 isbn = {9781510601413},
 issn = {1996756X},
 journal = {Optics, Photonics and Digital Technologies for Imaging Applications IV},
 pages = {989616},
 title = {{Deformably registering and annotating whole CLARITY brains to an atlas via masked LDDMM}},
 url = {https://doi.org/10.1117/12.2227444},
 volume = {9896},
 year = {2016}
}

@article{Kutten2016b,
 abstract = {CLARITY is a method for converting biological tissues into translucent and porous hydrogel-tissue hybrids. This facilitates interrogation with light sheet microscopy and penetration of molecular probes while avoiding physical slicing. In this work, we develop a pipeline for registering CLARIfied mouse brains to an annotated brain atlas. Due to the novelty of this microscopy technique it is impractical to use absolute intensity values to align these images to existing standard atlases. Thus we adopt a large deformation diffeomorphic approach for registering images via mutual information matching. Furthermore we show how a cascaded multi-resolution approach can improve registration quality while reducing algorithm run time. As acquired image volumes were over a terabyte in size, they were far too large for work on personal computers. Therefore the NeuroData computational infrastructure was deployed for multi-resolution storage and visualization of these images and aligned annotations on the web.},
 archiveprefix = {arXiv},
 arxivid = {1613.00356},
 author = {Kutten, Kwame S. and Charon, Nicolas and Miller, Michael I. and Ratnanather, J. Tilak and Matelsky, Jordan and Baden, Alexander D. and Lillaney, Kunal and Deisseroth, Karl and Ye, Li and Vogelstein, Joshua T.},
 author+an = {1=trainee;5=trainee;6=trainee;7=trainee;10=highlight},
 booktitle = {Medical Image Computing and Computer Assisted Intervention âˆ’ MICCAI 2017},
 doi = {10.1007/978-3-319-66182-7_32},
 editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
 eprint = {1612.00356},
 isbn = {9783319661810},
 issn = {16113349},
 journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
 pages = {275--282},
 title = {{A large deformation diffeomorphic approach to registration of CLARITY images via mutual information}},
 url = {https://link.springer.com/chapter/10.1007/978-3-319-66182-7_32},
 year = {2017}
}

@article{Lillaney2018,
 abstract = {We describe NDStore, a scalable multi-hierarchical data storage deployment for spatial analysis of neuroscience data on the AWS cloud. The system design is inspired by the requirement to maintain high I/O throughput for workloads that build neural connectivity maps of the brain from peta-scale imaging data using computer vision algorithms. We store all our data on the AWS object store S3 to limit our deployment costs. S3 serves as our base-tier of storage. Redis, an in-memory key-value engine, is used as our caching tier. The data is dynamically moved between the different storage tiers based on user access. All programming interfaces to this system are RESTful web-services. We include a performance evaluation that shows that our production system provides good performance for a variety of workloads by combining the assets of multiple cloud services.},
 author = {Lillaney, Kunal and Kleissas, Dean and Eusman, Alexander and Perlman, Eric and {Gray Roncal}, William and Vogelstein, Joshua T. and Burns, Randal},
 author+an = {1=trainee;6=highlight},
 doi = {10.1109/eScience.2018.00037},
 isbn = {9781538691564},
 journal = {Proceedings - IEEE 14th International Conference on eScience, e-Science 2018},
 keywords = {Big Data,Cloud Computing,Neuroscience,Object Storage,Spatial Data},
 pages = {223--233},
 title = {{Building NDStore through hierarchical storage management and microservice processing}},
 url = {https://ieeexplore.ieee.org/abstract/document/8588656},
 year = {2018}
}

@article{Mhembere2013,
 abstract = {Graphs are quickly emerging as a leading abstraction for the representation of data. One important application domain originates from an emerging discipline called 'connectomics'. Connectomics studies the brain as a graph; vertices correspond to neurons (or collections thereof) and edges correspond to structural or functional connections between them. To explore the variability of connectomes - to address both basic science questions regarding the structure of the brain, and medical health questions about psychiatry and neurology - one can study the topological properties of these brain-graphs. We define multivariate glocal graph invariants: these are features of the graph that capture various local and global topological properties of the graphs. We show that the collection of features can collectively be computed via a combination of daisy-chaining, sparse matrix representation and computations, and efficient approximations. Our custom open-source Python package serves as a back-end to a Web-service that we have created to enable researchers to upload graphs, and download the corresponding invariants in a number of different formats. Moreover, we built this package to support distributed processing on multicore machines. This is therefore an enabling technology for network science, lowering the barrier of entry by providing tools to biologists and analysts who otherwise lack these capabilities. As a demonstration, we run our code on 120 brain-graphs, each with approximately 16M vertices and up to 90M edges. {\textcopyright} 2013 IEEE.},
 author = {Mhembere, Disa and {Gray Roncal}, William and Sussman, Daniel and Priebe, Carey E. and Jung, Rex and Ryman, Sephira and Vogelstein, R. Jacob and Vogelstein, Joshua T. and Burns, Randal},
 author+an = {1=trainee;2=trainee;8=highlight},
 doi = {10.1109/GlobalSIP.2013.6736874},
 isbn = {9781479902484},
 journal = {2013 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2013 - Proceedings},
 month = {dec},
 pages = {297--300},
 title = {{Computing scalable multivariate glocal invariants of large (brain-) graphs}},
 url = {http://dx.doi.org/10.1109/GlobalSIP.2013.6736874},
 year = {2013}
}

@inproceedings{mhembere2017knor,
 address = {Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing},
 author = {Mhembere, Disa and Priebe, Carey E and Vogelstein, Joshua T and Burns, Randal},
 author+an = {1=trainee;3=highlight},
 booktitle = {Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing},
 isbn = {9781450346993},
 keywords = {acm reference format,and randal,carey e,cloud,clustering,da zheng,disa mhembere,joshua t,k-means,numa,parallel,priebe,semi-external memory,vogelstein},
 organization = {ACM},
 title = {{knor : A NUMA-Optimized In-Memory , Distributed and Semi-External-Memory k-means Library}},
 url = {https://arxiv.org/abs/1606.08905},
 year = {2017}
}

@article{Nikolaidis343392,
 abstract = {Increasing the reproducibility of neuroimaging measurement addresses a central impediment to the clinical impact of human neuroscience. Recent efforts demonstrating variance in functional brain organization within and between individuals shows a need for improving reproducibility of functional parcellations without long scan times. We apply bootstrap aggregation, or bagging, to the problem of improving reproducibility in functional parcellation. We use two large datasets to demonstrate that compared to a standard clustering framework, bagging improves the reproducibility and test-retest reliability of both cortical and subcortical functional parcellations across a range of sites, scanners, samples, scan lengths, and clustering parameters. With as little as six minutes of scan time bagging creates more reproducible parcellations than standard approaches with twice as much data. This suggests bagging may be a key method for improving functional parcellation and bringing functional neuroimaging-based measurement closer to clinical impact.},
 author = {Nikolaidis, Aki and Heinsfeld, Anibal Solon and Xu, Ting and Bellec, Pierre and Vogelstein, Joshua and Milham, Michael},
 author+an = {5=highlight},
 doi = {10.1101/343392},
 journal = {bioRxiv},
 month = {jun},
 pages = {343392},
 publisher = {Cold Spring Harbor Laboratory},
 title = {{Bagging Improves Reproducibility of Functional Parcellation of the Human Brain}},
 url = {http://biorxiv.org/content/early/2019/08/28/343392.abstract},
 year = {2019}
}

@article{Petralia2013,
 abstract = {Nonparametric estimation of the conditional distribution of a response given high- dimensional features is a challenging problem. It is important to allownot only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional. We propose a multiscale dictio- nary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights de- pendent on the path through a tree decomposition of the feature space. Afast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner. The algorithm scales efficiently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features. 1},
 author = {Petralia, Francesca and Vogelstein, Joshua and Dunson, David B},
 author+an = {2=highlight},
 isbn = {10.1186/2042-1001-1-16},
 issn = {10495258},
 journal = {Advances in Neural Information Processing Systems},
 pmid = {25233306},
 title = {{Multiscale Dictionary Learning for Estimating Conditional Distributions}},
 url = {https://papers.nips.cc/paper/4944-multiscale-dictionary-learning-for-estimating-conditional-distributions},
 year = {2013}
}

@inproceedings{tomita2017roflmao,
 abstract = {Random Forest (RF) remains one of the most widely used general purpose classification methods. Two recent largescale empirical studies demonstrated it to be the best overall classification method among a variety of methods evaluated. One of its main limitations, however, is that it is restricted to only axis-aligned recursive partitions of the feature space. Consequently, RF is particularly sensitive to the orientation of the data. Several studies have proposed "oblique" decision forest methods to address this limitation. However, these methods either have a time and space complexity significantly greater than RF, are sensitive to unit and scale, or empirically do not perform as well as RF on real data. One promising oblique method that was proposed alongside the canonical RF method, called Forest-RC (F-RC), has not received as much attention by the community. Despite it being just as old as RF, virtually no studies exist investigating its theoretical or empirical performance. In this work, we demonstrate that F-RC empirically outperforms RF and another recently proposed oblique method called Random Rotation Random Forest, while approximately maintaining the same computational complexity. Furthermore, a variant of F-RC which rank transforms the data prior to learning is especially invariant to affine transformations and robust to data corruption. Open source code is available.},
 author = {Tomita, Tyler M. and Maggioni, Mauro and Vogelstein, Joshua T.},
 author+an = {1=trainee;3=highlight},
 booktitle = {Proceedings of the 17th SIAM International Conference on Data Mining, SDM 2017},
 doi = {10.1137/1.9781611974973.56},
 isbn = {9781611974874},
 organization = {SIAM},
 pages = {498--506},
 title = {{ROFLMAO: Robust oblique forests with linear MAtrix operations}},
 year = {2017}
}

@inproceedings{Zheng2015,
 author = {Zheng, Da and Mhembere, Disa and Burns, Randal and Vogelstein, Joshua T and Priebe, Carey E and Szalay, Alexander S},
 author+an = {1=trainee;2=trainee;4=highlight},
 booktitle = {USENIX Conference on File and Storage Technologies},
 doi = {10.1109/ICDE.2012.28},
 eprint = {1408.0500},
 isbn = {9781931971201},
 issn = {10844627},
 title = {{FlashGraph: Processing Billion-Node Graphs on an Array of Commodity SSDs}},
 url = {http://arxiv.org/abs/1408.0500},
 year = {2012}
}

@article{zheng2016flashr,
 abstract = {R is one of the most popular programming languages for statistics and machine learning, but the R framework is relatively slow and unable to scale to large datasets. The general approach for speeding up an implementation in R is to implement the algorithms in C or FORTRAN and provide an R wrapper. FlashR takes a different approach: it executes R code in parallel and scales the code beyond memory capacity by utilizing solid-state drives (SSDs) automatically. It provides a small number of generalized operations (GenOps) upon which we reimplement a large number of matrix functions in the R base package. As such, FlashR parallelizes and scales existing R code with little/no modification. To reduce data movement between CPU and SSDs, FlashR evaluates matrix operations lazily, fuses operations at runtime, and uses cache-aware, two-level matrix partitioning. We evaluate FlashR on a variety of machine learning and statistics algorithms on inputs of up to four billion data points. FlashR out-of-core tracks closely the performance of FlashR in-memory. The R code for machine learning algorithms executed in FlashR outperforms the in-memory execution of H2O and Spark MLlib by a factor of 2-10 and outperforms Revolution R Open by more than an order of magnitude.},
 archiveprefix = {arXiv},
 arxivid = {1604.06414},
 author = {Zheng, Da and Mhembere, Disa and Vogelstein, Joshua T. and Priebe, Carey E. and Burns, Randal},
 author+an = {1=trainee;2=trainee;3=highlight},
 eprint = {1604.06414},
 journal = {PPoPP},
 month = {may},
 title = {{FlashR: R-Programmed Parallel and Scalable Machine Learning using SSDs}},
 url = {http://arxiv.org/abs/1604.06414},
 year = {2016}
}

